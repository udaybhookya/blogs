# üìù My AI & Tech Blog

Welcome to my blog repository! This space is where I write and share articles on topics I‚Äôm passionate about ‚Äî from Artificial Intelligence, Machine Learning and Gen AI!

## üîó Quick Links

- **Blog Home** ¬∑ [Medium](https://medium.com/@bhookyauday)
- **LinkedIn** - [LinkedIn Profile](https://www.linkedin.com/in/uday-chandra/)

## üöÄ What‚Äôs Inside

1. **Statistical Significance Tests: A statistical way to compare data populations**  
   **Abstract:**  
   This blog explains how to statistically compare data populations using significance tests. It begins by showing the limitations of using mean, median, and visualization tools like box plots and histograms for comparing data similarity. It then introduces Student's t-test as a robust method to measure differences between samples, supported by concepts like t-value, p-value, and hypothesis testing. The blog also outlines when to use variations like paired, independent, and one-sample t-tests. Additionally, it covers Z-test, Chi-square, ANOVA, and KS test, offering a practical guide to selecting the right statistical method based on the data scenario.
   
   **Read more:** [Statistical Significance Tests: A statistical way to compare data populations](https://medium.com/@igniobydigitate/statistical-significance-tests-a-statistical-way-to-compare-data-populations-1effad328f7c)


2. **How We Met AI: Episode 1 ‚Äì RNN Encoder‚ÄìDecoder Revolution**  
   **Abstract:**  
   This post explores how two simple RNNs revolutionized machine translation. We cover the limitations of rule-based systems and basic neural nets, introduce RNNs for sequence modeling, and explain the encoder‚Äìdecoder approach that turns any-length sentence into a fixed ‚Äúcontext vector‚Äù before reconstructing it word by word in another language. You‚Äôll also learn training techniques like teacher forcing and why challenges like vanishing gradients led to LSTM and GRU.
   
   **Read more:** [How We Met AI: Episode 1 ‚Äì RNN Encoder‚ÄìDecoder Revolution](https://medium.com/@bhookyauday/how-we-met-ai-episode-1-rnn-encoder-decoder-revolution-a1ffabef76d5)


3. **How We Met AI: Episode 2 - From Gates to Memory**
   **Abstract:**
   In this sequel to the RNN revolution, we dive into the core challenge that crippled vanilla RNNs ‚Äî the vanishing and exploding gradient problem. This blog explains why traditional RNNs struggle with long sequences, and how LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) networks introduced a game-changing solution: gated memory. With an intuitive walkthrough and real-world metaphors, we explore how these networks remember better, forget smarter, and paved the way for modern sequence modeling.

   **Read more:** [How We Met AI: Episode 2: From Gates to Memory](https://medium.com/@bhookyauday/how-we-met-ai-episode-2-from-gates-to-memory-how-lstm-gru-conquered-the-vanishing-gradient-453b46cf5636)
 

4. **How We Met AI: Episode 3:**  
   _Coming soon‚Ä¶_


